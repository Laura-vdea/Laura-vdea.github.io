VEDA 바로가기 : `www.vedacademy.co.kr`
VEDA(한화비전 아카데미) 영상으로 확인하기 : `https://url.kr/zy9afd`
본 후기는 VEDA(한화비전 아카데미) 첫번째 학습 기록으로 작성되었습니다.
교육기간 : `2024.07.15~2024.12.16`

------- 
- 작성일 : 2024-09-08
- 작성자 : 박지우
- 수업할 때에 사용한 언어 : C/C++
- 포스팅 목적 : 8주 차시의 한화비전 베다 수업 회고록

---

![](https://velog.velcdn.com/images/laura_vdea/post/bcd5298c-6c85-409e-a633-f92e54247c85/image.png)


------

# 목차

1. [서론](#0-서론)
2. [네트워크 구조 및 사용 기술](#1-네트워크-구조-및-사용-기술)
3. [서버 프로그램 설명 (epoll 기반)](#2-서버-프로그램-설명-epoll-기반)
4. [클라이언트 프로그램 설명 (select 기반)](#3-클라이언트-프로그램-설명-select-기반)
5. [스레드와 동기화 코드 정리](#4-스레드와-동기화-코드-정리)
6. [개인 프로젝트 적용](#5-개인-프로젝트-적용)
7. [개인 프로젝트 스마트포인터 C구현](#6-개인-프로젝트-스마트포인터-c구현)
7. [BSP 진도가 모두 끝난 뒤에 느낀 점](#7-bsp-진도가-모두-끝난-뒤에-느낀-점)
8. [마무리](#8-마무리)



------
## 0. 서론
이번 주는 네트워크 프로그래밍을 학습하면서 다중 클라이언트와 서버 간의 통신을 처리하는 방법에 대해 집중적으로 배웠다. 특히, select와 epoll을 사용하여 클라이언트와 서버 간의 비동기 통신을 구현하는 방법에 대해 분석하고, 스마트 포인터(SmartPtr)를 사용해 메모리 관리와 자원 해제를 구현을 한 번 직접 해보았다.
이번 주는 간단하게 포스팅 하려고 한다.
_**
이 프로그램은 다음과 같은 주요 기능을 다룬다.**_

    다중 클라이언트 접속 관리
    비동기 메시지 처리
    스마트 포인터를 사용한 메모리 관리
    클라이언트-서버 간 브로드캐스트 메시지 전송
    
**채팅 프로그램은 아래 노션에도 정리를 해둔 바가 있다.**
https://siatjiwoointrosite.notion.site/select-44e8dd174c0a4dd19ea636be0c6e0f88?pvs=4

##### mutex
![](https://velog.velcdn.com/images/laura_vdea/post/b1c41883-152e-4877-a510-bb186be36104/image.png)

##### semaphore
![](https://velog.velcdn.com/images/laura_vdea/post/2737f7bd-3749-4852-86f4-29842c6da754/image.png)

link : https://learn.microsoft.com/ko-kr/windows-hardware/drivers/kernel/introduction-to-mutex-objects

------

## 1. 네트워크 구조 및 사용 기술
이 작은 채팅 프로그램은 `C언어`로 작성되었으며, 네트워크 소켓 프로그래밍을 위한 주요 헤더 파일과 함수들을 사용한다.

    socket(), connect(), bind(), listen(), accept() 등의 네트워크 소켓 함수들이 사용되었다.
    select()와 epoll()을 사용하여 비동기적으로 여러 클라이언트의 연결과 메시지를 처리한다.
    pthread를 사용하여 각 클라이언트에 대해 스레드 기반의 뮤텍스를 적용하여 자원 접근을 동기화를 하였다.
    스마트 포인터는 클라이언트의 자원(파일 디스크립터 및 뮤텍스)을 효율적으로 관리한다.
    
#### 서버 코드
```
#include <stdio.h>
#include <string.h>
#include <stdlib.h>
#include <unistd.h>
#include <sys/socket.h>
#include <arpa/inet.h>
#include <sys/epoll.h>
#include <stdarg.h>
#include <fcntl.h>
#include <pthread.h>

#define DEFAULT_TCP_PORT 5100
#define MAX_EVENTS 32
#define MAX_CLIENTS 10
#define BUFFER_SIZE 1024

// SmartPtr 구조체 및 함수 선언
typedef struct SmartPtr {
    void *ptr;
    int *ref_count;
    pthread_mutex_t *mutex;
} SmartPtr;

SmartPtr create_smart_ptr(void *ptr);
void retain(SmartPtr *sp);
void release(SmartPtr *sp);

// 스마트 포인터 클라이언트 관리 함수
typedef struct {
    int client_fd;
    int client_id;
    pthread_mutex_t *client_mutex; // 클라이언트 별 뮤텍스
} ClientInfo;

// 클라이언트 정보 스마트 포인터 배열
SmartPtr client_infos[MAX_CLIENTS];

// 서버 함수 선언
int create_network_tcp_process(int num_tcp_proc, ...);
void setnonblocking(int fd);
void broadcast_message(int sender_fd, char *message, int message_len, int max_clients, int is_server);

int main() {
    printf("VEDA MULTI-CLIENT CHAT PROGRAM (with SmartPtr)\n");
    usleep(1000);
    create_network_tcp_process(1, "127.0.0.1", DEFAULT_TCP_PORT);
    return 0;
}

// Smart pointer 생성 함수 구현
SmartPtr create_smart_ptr(void *ptr) {
    SmartPtr sp;
    sp.ptr = ptr;
    sp.ref_count = (int *)malloc(sizeof(int));
    if (sp.ref_count == NULL) {
        perror("Failed to allocate memory for ref_count");
        exit(EXIT_FAILURE);
    }
    *(sp.ref_count) = 1;
    sp.mutex = (pthread_mutex_t *)malloc(sizeof(pthread_mutex_t));
    if (sp.mutex == NULL) {
        perror("Failed to allocate memory for mutex");
        free(sp.ref_count);
        exit(EXIT_FAILURE);
    }
    pthread_mutex_init(sp.mutex, NULL);
    return sp;
}

// Smart pointer retain 함수 구현
void retain(SmartPtr *sp) {
    pthread_mutex_lock(sp->mutex);
    (*(sp->ref_count))++;
    pthread_mutex_unlock(sp->mutex);
}

// Smart pointer release 함수 구현
void release(SmartPtr *sp) {
    int should_free = 0;
    pthread_mutex_lock(sp->mutex);
    (*(sp->ref_count))--;
    if (*(sp->ref_count) == 0) {
        should_free = 1;
    }
    pthread_mutex_unlock(sp->mutex);

    if (should_free) {
        free(sp->ptr);
        free(sp->ref_count);
        pthread_mutex_destroy(sp->mutex);
        free(sp->mutex);
    }
}

void setnonblocking(int fd) {
    int flags = fcntl(fd, F_GETFL, 0);
    if (flags == -1) {
        perror("fcntl F_GETFL");
        exit(EXIT_FAILURE);
    }
    if (fcntl(fd, F_SETFL, flags | O_NONBLOCK) == -1) {
        perror("fcntl F_SETFL");
        exit(EXIT_FAILURE);
    }
}

// 클라이언트 관리 및 서버 코드 수정
int create_network_tcp_process(int num_tcp_proc, ...) {
    va_list args;
    va_start(args, num_tcp_proc);

    for (int i = 0; i < num_tcp_proc; i++) {
        int ssock, epfd, nfds = 0, client_count = 1;
        socklen_t clen;
        struct sockaddr_in servaddr, cliaddr;
        struct epoll_event ev, events[MAX_EVENTS];
        char buffer[BUFFER_SIZE];
        char client_ip[INET_ADDRSTRLEN];

        const char *ip_address = va_arg(args, const char*);
        int port = va_arg(args, int);

        // 서버 소켓 생성
        if ((ssock = socket(AF_INET, SOCK_STREAM, 0)) < 0) {
            perror("socket()");
            return -1;
        }

        // 소켓 재사용 설정
        int enable = 1;
        if (setsockopt(ssock, SOL_SOCKET, SO_REUSEADDR, &enable, sizeof(int)) < 0) {
            perror("setsockopt(SO_REUSEADDR) failed");
            return -1;
        }

        memset(&servaddr, 0, sizeof(servaddr));
        servaddr.sin_family = AF_INET;
        servaddr.sin_addr.s_addr = INADDR_ANY;
        servaddr.sin_port = htons(port);

        // 소켓 바인딩
        if (bind(ssock, (struct sockaddr *)&servaddr, sizeof(servaddr)) < 0) {
            perror("bind()");
            return -1;
        }

        // 클라이언트 연결 대기
        if (listen(ssock, 8) < 0) {
            perror("listen()");
            return -1;
        }

        printf("서버가 클라이언트의 연결을 기다립니다...\n");

        epfd = epoll_create(MAX_EVENTS);
        if (epfd == -1) {
            perror("epoll_create");
            return -1;
        }

        ev.events = EPOLLIN;
        ev.data.fd = ssock;

        if (epoll_ctl(epfd, EPOLL_CTL_ADD, ssock, &ev) == -1) {
            perror("epoll_ctl: listen_sock");
            return -1;
        }

        // 서버의 표준 입력(STDIN)을 epoll에 추가
        ev.events = EPOLLIN;
        ev.data.fd = STDIN_FILENO;
        if (epoll_ctl(epfd, EPOLL_CTL_ADD, STDIN_FILENO, &ev) == -1) {
            perror("epoll_ctl: stdin");
            return -1;
        }

        while (1) {
            nfds = epoll_wait(epfd, events, MAX_EVENTS, -1);
            for (int i = 0; i < nfds; i++) {
                if (events[i].data.fd == ssock) {
                    // 새 클라이언트 연결 처리
                    clen = sizeof(cliaddr);
                    int csock = accept(ssock, (struct sockaddr *)&cliaddr, &clen);
                    if (csock > 0) {
                        inet_ntop(AF_INET, &cliaddr.sin_addr, client_ip, INET_ADDRSTRLEN);
                        printf("[ 클라이언트 %d가 연결되었습니다. IP: %s ]\n", client_count, client_ip);

                        // 각 클라이언트에 대한 뮤텍스 생성
                        pthread_mutex_t *client_mutex = (pthread_mutex_t *)malloc(sizeof(pthread_mutex_t));
                        pthread_mutex_init(client_mutex, NULL);

                        setnonblocking(csock);
                        ev.events = EPOLLIN | EPOLLET;
                        ev.data.fd = csock;

                        if (epoll_ctl(epfd, EPOLL_CTL_ADD, csock, &ev) == -1) {
                            perror("epoll_ctl: add");
                            return -1;
                        }

                        // 클라이언트 소켓 및 ID 스마트 포인터로 관리
                        ClientInfo *client_info = (ClientInfo *)malloc(sizeof(ClientInfo));
                        client_info->client_fd = csock;
                        client_info->client_id = client_count++;
                        client_info->client_mutex = client_mutex; // 클라이언트 별 뮤텍스

                        client_infos[csock] = create_smart_ptr(client_info);

                        // 클라이언트 시작 시 뮤텍스 호출
                        pthread_mutex_lock(client_info->client_mutex);
                        printf("뮤텍스 %d 호출\n", client_info->client_id);
                        pthread_mutex_unlock(client_info->client_mutex);
                    }
                } 
                
                else if (events[i].data.fd == STDIN_FILENO) {
                    // 서버 입력 처리
                    fgets(buffer, BUFFER_SIZE, stdin);

                    if (strncmp(buffer, "exit", 4)==0) {
                        printf("서버 종료\n");
                        break;
                    }

                    if (strncmp(buffer, "...", 3) ==0) {
                        close(ssock);
                        exit(1);
                        break;
                    }

                    if(strlen(buffer) < 0) {
                        buffer[strlen(buffer) - 1] = '\0';
                    }

                    if (strlen(buffer) > 0) {
                        broadcast_message(-1, buffer, strlen(buffer), MAX_CLIENTS, 1);
                        buffer[strcspn(buffer, "\n")] = 0;
                        printf("[내가 보낼 메시지]: ");
                        printf("%s\n", buffer);
                    }
                } 
                
                else if (events[i].events & EPOLLIN) {
                    // 클라이언트로부터 메시지 수신 처리
                    int csock = events[i].data.fd;
                    memset(buffer, 0, BUFFER_SIZE);
                    int n = read(csock, buffer, BUFFER_SIZE);

                    if (n > 0) {
                        ClientInfo *client_info = (ClientInfo *)client_infos[csock].ptr;
                        printf("[ 클라이언트 %d 메세지 ]: %s", client_info->client_id, buffer);
                        broadcast_message(csock, buffer, n, MAX_CLIENTS, 0);
                    } else if (n == 0) {
                        // 클라이언트가 종료될 때 처리
                        ClientInfo *client_info = (ClientInfo *)client_infos[csock].ptr;
                        printf("클라이언트 %d번 연결이 종료되었습니다. 소켓: %d번\n", client_info->client_id, csock);

                        // 클라이언트 종료 시 뮤텍스 잠금
                        pthread_mutex_lock(client_info->client_mutex);
                        printf("뮤텍스 %d 잠금\n", client_info->client_id);
                        pthread_mutex_unlock(client_info->client_mutex);

                        close(csock);
                        events[i].data.fd = -1;
                        release(&client_infos[csock]);
                    }
                }

            }
        }

        va_end(args);
        close(ssock);
    }
    return 0;
}

// 메시지 브로드캐스트
void broadcast_message(int sender_fd, char *message, int message_len, int max_clients, int is_server) {
    char broadcast_message[BUFFER_SIZE + 50];

    if (is_server) {
        // 서버에서 보낸 메시지
        snprintf(broadcast_message, sizeof(broadcast_message), "[서버 메시지]: %s", message);
    } else {
        // 클라이언트에서 보낸 메시지
        ClientInfo *client_info = (ClientInfo *)client_infos[sender_fd].ptr;
        snprintf(broadcast_message, sizeof(broadcast_message), "[클라이언트 %d 메시지]: %s", client_info->client_id, message);
    }

    // 모든 클라이언트에게 전송
    for (int i = 0; i < MAX_CLIENTS; i++) {
        if (client_infos[i].ptr != NULL && ((ClientInfo *)client_infos[i].ptr)->client_fd != sender_fd) {
            write(((ClientInfo *)client_infos[i].ptr)->client_fd, broadcast_message, strlen(broadcast_message));
        }
    }
}
```
#### 클라이언트 코드
```
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <arpa/inet.h>
#include <sys/select.h>

#define PORT 5100

int main(int argc, char *argv[]) {
    int sock = 0;
    struct sockaddr_in serv_addr;
    char buffer[1024];
    fd_set readfds;
    int max_sd;

    // 명령행 인수 확인 (IP 주소 입력 여부 확인)
    if (argc != 2) {
        printf("사용법: %s <서버 IP 주소>\n", argv[0]);
        return -1;
    }

    // 소켓 생성
    if ((sock = socket(AF_INET, SOCK_STREAM, 0)) < 0) {
        printf("소켓 생성 오류\n");
        return -1;
    }

    serv_addr.sin_family = AF_INET;
    serv_addr.sin_port = htons(PORT);

    // 서버 주소 설정 (명령행 인수로 받은 IP 주소 사용)
    if (inet_pton(AF_INET, argv[1], &serv_addr.sin_addr) <= 0) {
        printf("잘못된 주소\n");
        return -1;
    }

    // 서버에 연결
    if (connect(sock, (struct sockaddr *)&serv_addr, sizeof(serv_addr)) < 0) {
        printf("연결 실패\n");
        return -1;
    }

    printf("서버와 연결되었습니다.\n");
    
    while (1) {
        FD_ZERO(&readfds);
        FD_SET(0, &readfds);  // 표준 입력 감시 (키보드 입력)
        FD_SET(sock, &readfds);  // 서버 소켓 감시
        max_sd = sock;

        int activity = select(max_sd + 1, &readfds, NULL, NULL, NULL);

        if (activity < 0) {
            perror("select()");
            break;
        }

        // 사용자 입력 감지
        if (FD_ISSET(0, &readfds)) {
            fgets(buffer, 1024, stdin);

            if (strncmp(buffer, "exit", 4) == 0) {
                printf("채팅 종료\n");
                close(sock);  // 소켓을 닫고 종료
                break;
            }

            if (strncmp(buffer, "...", 3) == 0) {
                close(sock);
                printf("클라이언트 연결 종료\n");
                break;
            }

            send(sock, buffer, strlen(buffer), 0);

            if (strlen(buffer) > 0) {
                printf("[내가 보낸 메시지]: %s", buffer);
            }
        }

        // 서버로부터 메시지 수신
        if (FD_ISSET(sock, &readfds)) {
            int valread = read(sock, buffer, 1024);
            if (valread <= 0) {
                printf("서버 연결이 종료되었습니다.\n");
                break;
            }

            buffer[valread] = '\0';
            printf("%s", buffer);  // 서버로부터 받은 메시지를 그대로 출력
        }
    }

    close(sock);
    return 0;
}
```
------
    
## 2. 서버 프로그램 설명 (epoll 기반)
#### 2.1 서버 소켓 생성 및 설정

 서버는 먼저 `TCP 소켓`을 생성하고, 지정된 IP 주소와 포트 번호로 바인딩한 후 클라이언트의 연결 요청을 대기한다. 
 이 과정에서 서버는 `SO_REUSEADDR` 옵션을 설정하여, 소켓이 종료되었을 때 바로 재사용할 수 있도록 설정하였다.

```
if ((ssock = socket(AF_INET, SOCK_STREAM, 0)) < 0) {
    perror("socket()");
    return -1;
}

int enable = 1;
if (setsockopt(ssock, SOL_SOCKET, SO_REUSEADDR, &enable, sizeof(int)) < 0) {
    perror("setsockopt(SO_REUSEADDR) failed");
    return -1;
}
```

#### 2.2 epoll을 사용한 다중 클라이언트 처리

 서버는 클라이언트의 연결과 메시지 수신을 epoll을 통해 비동기적으로 처리한다. 
 `epoll_create()`로 epoll 인스턴스를 생성한 후, 서버 소켓을 `EPOLL_CTL_ADD`로 `epoll`에 등록하여 클라이언트 연결을 감시한다. 
 이후 `epoll_wait()`를 사용해 이벤트가 발생할 때까지 대기한다.

```
epfd = epoll_create(MAX_EVENTS);
if (epfd == -1) {
    perror("epoll_create");
    return -1;
}

ev.events = EPOLLIN;
ev.data.fd = ssock;

if (epoll_ctl(epfd, EPOLL_CTL_ADD, ssock, &ev) == -1) {
    perror("epoll_ctl: listen_sock");
    return -1;
}
```

#### 2.3 클라이언트 연결 및 메시지 처리

클라이언트가 서버에 연결되면 `accept()`를 호출하여 연결을 처리하고, 새로 연결된 클라이언트의 소켓을 epoll에 등록한다. 클라이언트의 메시지가 수신되면 이를 읽고, 연결된 모든 클라이언트에게 브로드캐스트 메시지로 전달한다.

```
if (events[i].data.fd == ssock) {
    clen = sizeof(cliaddr);
    int csock = accept(ssock, (struct sockaddr *)&cliaddr, &clen);
    if (csock > 0) {
        printf("[ 클라이언트 %d가 연결되었습니다. ]\n", client_count);
        setnonblocking(csock);
        ev.events = EPOLLIN | EPOLLET;
        ev.data.fd = csock;
        epoll_ctl(epfd, EPOLL_CTL_ADD, csock, &ev);
    }
}
```

#### 2.4 스마트 포인터를 이용한 클라이언트 관리

 각 클라이언트의 정보를 스마트 포인터로 관리하여, 클라이언트가 연결을 종료하면 자동으로 메모리를 해제한다. 
 스마트 포인터는 클라이언트의 파일 디스크립터와 뮤텍스를 관리하며, 참조 카운팅을 통해 메모리를 효율적으로 관리한다.

```
SmartPtr create_smart_ptr(void *ptr) {
    SmartPtr sp;
    sp.ptr = ptr;
    sp.ref_count = (int *)malloc(sizeof(int));
    *(sp.ref_count) = 1;
    sp.mutex = (pthread_mutex_t *)malloc(sizeof(pthread_mutex_t));
    pthread_mutex_init(sp.mutex, NULL);
    return sp;
}
```

---------

## 3. 클라이언트 프로그램 설명 (select 기반)
#### 3.1 소켓 생성 및 서버 연결

클라이언트는 서버와의 통신을 위해 TCP 소켓을 생성한 후, 지정된 서버 IP와 포트로 연결을 시도한다.

```
if ((sock = socket(AF_INET, SOCK_STREAM, 0)) < 0) {
    printf("소켓 생성 오류\n");
    return -1;
}

if (connect(sock, (struct sockaddr *)&serv_addr, sizeof(serv_addr)) < 0) {
    printf("연결 실패\n");
    return -1;
}
```

#### 3.2 select를 이용한 입력 및 메시지 처리

클라이언트는 `select()`를 이용해 표준 입력(키보드 입력)과 서버로부터의 메시지를 동시에 감시한다. `select()`는 파일 디스크립터 셋을 사용해 입력을 감지하고, 이벤트가 발생하면 이를 처리한다.

```
FD_ZERO(&readfds);
FD_SET(0, &readfds);  // 키보드 입력 감시
FD_SET(sock, &readfds);  // 서버 소켓 감시

int activity = select(sock + 1, &readfds, NULL, NULL, NULL);

if (FD_ISSET(0, &readfds)) {
    fgets(buffer, 1024, stdin);
    send(sock, buffer, strlen(buffer), 0);
}
```

------------

## 4. 스레드와 동기화 코드 정리
 스레드와 동기화 관련 프로그램을 다루며, 각 프로그램에서 중요한 동작을 정리하고자 한다. 
각 코드에서는 `pthread` 라이브러리를 이용한 멀티스레딩 구현과 함께, `mutex`, `semaphore`, 그리고 `signal`을 통해 스레드 간의 동기화를 구현한 다양한 예제를 다룬다. 각 예제는 스레드 동작을 명확히 이해하기 위한 코드로, commonCounter라는 전역 변수를 공유하며, 이를 각기 다른 방법으로 보호하는 방식에 차이를 두고 있다.

#### 4.1. mutex.c - 뮤텍스를 사용한 동기화

> pthread_mutex를 사용하여 두 개의 스레드가 동일한 자원인 commonCounter에 접근하는 동작을 동기화합니다. 뮤텍스는 스레드 간 자원 접근 순서를 보장하여, 동시에 여러 스레드가 접근할 때 발생할 수 있는 데이터 충돌을 방지합니다.

**주요 동작**

>    pthread_mutex_init()으로 뮤텍스를 초기화합니다.
    두 개의 스레드가 pthread_create()로 생성되어 inc_thread() 함수를 실행합니다.
    각 스레드는 pthread_mutex_lock()으로 자원 접근 전에 잠금을 걸고, pthread_mutex_unlock()으로 잠금을 해제합니다.
    마지막으로, pthread_mutex_destroy()로 뮤텍스를 소멸시킵니다.

```
#include <sys/types.h>
#include <unistd.h>
#include <stdio.h>
#include <stdlib.h>
#include <pthread.h>
#include <errno.h>
#include <string.h>

#define LOOP_MAX 10

int commonCounter = 0;
pthread_mutex_t mutexid;

void *inc_thread(void *);

int main(void) {
    pthread_t tid1, tid2;

    // mutex 생성
    pthread_mutex_init(&mutexid, NULL);

    if (pthread_create(&tid1, NULL, inc_thread, "thread1") || 
        pthread_create(&tid2, NULL, inc_thread, "thread2")) {
        perror("pthread_create");
        exit(errno);
    }

    pthread_join(tid1, NULL);
    pthread_join(tid2, NULL);

    // mutex 소멸
    pthread_mutex_destroy(&mutexid);

    return 0;
}

void *inc_thread(void *arg) {
    int loopCount, temp, i;
    char buffer[80];

    for (loopCount = 0; loopCount < LOOP_MAX; loopCount++) {
        // mutex lock
        pthread_mutex_lock(&mutexid);

        sprintf(buffer, "<%s> Common counter : from %d to ", (char *)arg, commonCounter);
        write(1, buffer, strlen(buffer));

        temp = commonCounter;
        for (i = 0; i < 900000; i++);  // delay
        commonCounter = temp + 1;

        sprintf(buffer, "%d\n", commonCounter);
        write(1, buffer, strlen(buffer));

        // mutex unlock
        pthread_mutex_unlock(&mutexid);
    }

    pthread_exit(NULL);
}
```

#### 4.2. semaphore.c - 세마포어를 사용한 동기화
> 세마포어는 뮤텍스와 비슷하지만, 하나 이상의 스레드가 공유 자원에 접근할 수 있는지를 제어합니다. 이 예제에서는 sem_wait()와 sem_post()를 통해 스레드가 자원을 잠그고, 해제하는 방식으로 동기화를 구현합니다.

**주요 동작**

> sem_init()으로 세마포어를 초기화합니다.
    두 개의 스레드가 pthread_create()로 생성되어 inc_thread() 함수를 실행합니다.
    각 스레드는 sem_wait()로 자원 접근 전에 세마포어를 잠그고, sem_post()로 해제합니다.
    sem_destroy()로 세마포어를 소멸시킵니다.

```
#include <sys/types.h>
#include <unistd.h>
#include <stdio.h>
#include <stdlib.h>
#include <pthread.h>
#include <errno.h>
#include <string.h>
#include <semaphore.h>

#define LOOP_MAX 10

int commonCounter = 0;
sem_t semid;

void *inc_thread(void *arg);

int main(void) {
    pthread_t tid1, tid2;

    // 세마포어 초기화
    sem_init(&semid, 0, 1);

    if (pthread_create(&tid1, NULL, inc_thread, "thread1") || 
        pthread_create(&tid2, NULL, inc_thread, "thread2")) {
        perror("pthread_create");
        exit(errno);
    }

    pthread_join(tid1, NULL);
    pthread_join(tid2, NULL);

    sem_destroy(&semid);  // 세마포어 삭제
    return 0;
}

void *inc_thread(void *arg) {
    int loopCount, temp, i;
    char buffer[80];

    for (loopCount = 0; loopCount < LOOP_MAX; loopCount++) {
        // 세마포어 잠금
        sem_wait(&semid);

        sprintf(buffer, "<%s> Common counter : from %d to ", (char *)arg, commonCounter);
        write(1, buffer, strlen(buffer));

        temp = commonCounter;
        for (i = 0; i < 900000; i++);  // delay
        commonCounter = temp + 1;

        sprintf(buffer, "%d\n", commonCounter);
        write(1, buffer, strlen(buffer));

        // 세마포어 잠금 해제
        sem_post(&semid);
    }

    pthread_exit(NULL);
}
```

#### 4.3. pthread.c - 스레드 비동기화
> 이 코드는 동기화 없이 두 개의 스레드가 공유 자원에 접근하는 동작을 보여줍니다. 동기화 없이 두 스레드가 공용 변수를 업데이트하므로, 경쟁 상태(race condition)가 발생할 수 있습니다.

**주요 동작**

  >    두 개의 스레드가 공용 변수를 증가시키며, 결과적으로 변수 값이 올바르게 증가하지 않을 수 있습니다.
      스레드 동기화의 중요성을 이해할 수 있는 예제입니다.

```
#include <sys/types.h>
#include <unistd.h>
#include <stdio.h>
#include <stdlib.h>
#include <pthread.h>
#include <errno.h>
#include <string.h>

#define LOOP_MAX 10

int commonCounter = 0;

void *inc_thread(void *);

int main(void) {
    pthread_t tid1, tid2;

    if (pthread_create(&tid1, NULL, inc_thread, "thread1") || 
        pthread_create(&tid2, NULL, inc_thread, "thread2")) {
        perror("pthread_create");
        exit(errno);
    }

    pthread_join(tid1, NULL);
    pthread_join(tid2, NULL);

    return 0;
}

void *inc_thread(void *arg) {
    int loopCount, temp, i;
    char buffer[80];

    for (loopCount = 0; loopCount < LOOP_MAX; loopCount++) {
        sprintf(buffer, "<%s> Common counter : from %d to ", (char *)arg, commonCounter);
        write(1, buffer, strlen(buffer));

        temp = commonCounter;
        for (i = 0; i < 900000; i++);  // delay
        commonCounter = temp + 1;

        sprintf(buffer, "%d\n", commonCounter);
        write(1, buffer, strlen(buffer));
    }

    pthread_exit(NULL);
}
```

#### 4.4. pthread_dedicated_signal.c - 스레드 기반 시그널 처리
> 이 프로그램은 별도의 시그널 처리 스레드를 만들어, SIGUSR1 및 SIGUSR2 시그널을 처리합니다. sigwait()를 사용하여 스레드에서 시그널을 대기하고, 시그널이 발생하면 이를 처리하는 방식입니다.

**주요 동작**

> sigwait()를 통해 시그널을 감지하고 처리합니다.
    별도의 스레드에서 시그널 처리만 담당하며, 다른 스레드는 각자의 작업을 병렬로 수행합니다.
    시그널 스레드가 SIGUSR1 또는 SIGUSR2 시그널을 처리할 때 콘솔에 메시지를 출력합니다.

```
#include <stdio.h>
#include <sys/types.h>
#include <unistd.h>
#include <signal.h>
#include <pthread.h>
#include <stdlib.h>

void* thread_function(void *x) {
    int i;
    for (i = 0; i < 10; ++i) {
        printf("thread %d (loop #%d)\n", *((int *) x), i);
        sleep(3);
    }
    return NULL;
}

void* signal_thread(void* arg) {
    sigset_t sigset;
    int sig;

    sigemptyset(&sigset);
    sigaddset(&sigset, SIGUSR1);
    sigaddset(&sigset, SIGUSR2);

    while (1) {
        sigwait(&sigset, &sig);
        if (sig == SIGUSR1) {
            printf("---> caught signal SIGUSR1\n");
        } else if (sig == SIGUSR2) {
            printf("---> caught signal SIGUSR2\n");
        } else {
            printf("unrecognized signal\n");
        }
    }
}

int main(void) {
    pthread_t sigtid, t1, t2;
    int n1 = 1, n2 = 2, rc;
    sigset_t sigset;

    sigfillset(&sigset);
    sigdelset(&sigset, SIGINT);
    pthread_sigmask(SIG_SETMASK, &sigset, NULL);

    if ((rc = pthread_create(&sigtid, NULL, signal_thread, NULL)) != 0) {
        fprintf(stderr, "pthread_create(signal_thread) : rc = %d\n", rc);
        exit(1);
    }
    if ((rc = pthread_create(&t1, NULL, thread_function, &n1)) != 0) {
        fprintf(stderr, "pthread_create(thread1) : rc = %d\n", rc);
        exit(1);
    }
    if ((rc = pthread_create(&t2, NULL, thread_function, &n2)) != 0) {
        fprintf(stderr, "pthread_create(thread2) : rc = %d\n", rc);
        exit(1);
    }

    pthread_join(t1, NULL);
    pthread_join(t2, NULL);
    pthread_join(sigtid, NULL);

    return 0;
}
```

------------

## 5. 개인 프로젝트 적용
### QT_Kernel_OS 프로젝트의 `Engine`에서 뮤텍스와 세마포어의 활용

`kernel_engine.c`에서는 프로세스 관리가 중점적으로 다뤄진다. 단일 프로세스 생성과 다중 프로세스 생성에 대한 함수들이 정의되어 있으며, 이를 통해 여러 프로세스를 동시에 생성하여 병렬 작업을 수행한다.

#### 5.1 단일 프로세스 생성
```
void create_single_process(void (*func)()) {
    pid_t pid = fork();
    if (pid == 0) {
        func();  // 자식 프로세스에서 함수 실행
        exit(EXIT_SUCCESS);
    } else {
        wait(NULL);  // 부모 프로세스가 자식 프로세스 대기
    }
}
```

`create_single_process()`는 `fork()`를 사용하여 자식 프로세스를 생성하며, 자식 프로세스는 주어진 함수를 실행하고 종료된다. 부모 프로세스는 자식이 종료될 때까지 대기한다.

#### 5.2 다중 프로세스 생성

```
void create_multi_processes(int num_processes, ...) {
    va_list args;
    va_start(args, num_processes);

    for (int i = 0; i < num_processes; i++) {
        pid_t pid = fork();
        if (pid == 0) {
            void (*process_func)() = va_arg(args, void (*)());
            process_func();  // 자식 프로세스에서 함수 실행
            exit(EXIT_SUCCESS);
        }
    }

    for (int i = 0; i < num_processes; i++) {
        wait(NULL);  // 모든 자식 프로세스 대기
    }

    va_end(args);
}
```
`create_multi_processes()`는 가변 인수 리스트를 사용하여 여러 프로세스를 생성하며, 각 프로세스는 할당된 함수를 실행한다. 병렬 처리가 가능하게 하여 시스템 성능을 극대화할 수 있다.

#### 5.3 고급 오류 처리 및 시스템 호출 시뮬레이션
`kernel_engine.c` 파일에서는 오류 처리와 시스템 호출 시뮬레이션도 중요한 요소다. 이 파일은 커널 레벨에서 발생할 수 있는 다양한 오류 상황을 처리하고, 시스템 호출을 통해 커널 동작을 모방하는 기능을 포함하고 있다.

#### 5.4 고급 오류 처리
 사용하는 고급 오류 처리 함수는 오류 발생 시 명확한 에러 메시지를 출력하고, 프로그램이 안전하게 종료되도록 보장한다. 예를 들어, kernel_errExit() 함수는 시스템 호출 에러나 메모리 할당 실패 같은 치명적인 오류가 발생할 때 호출된다.

```
  void kernel_errExit(const char *format, ...) {
    va_list argList;
    va_start(argList, format);
    outputError(true, errno, true, format, argList); // 에러 메시지 출력
    va_end(argList);
    terminate(true);  // 안전한 프로그램 종료
}
```

 이 함수는 내부적으로 outputError() 함수를 통해 오류를 출력한 후, terminate() 함수를 호출하여 프로그램을 안전하게 종료한다. 이를 통해 예기치 않은 오류로부터 시스템이 손상되는 것을 방지하고, 오류가 발생한 위치와 원인을 정확하게 추적할 수 있다.
 
#### 5.5 시스템 호출 시뮬레이션 
 시스템 호출은 운영체제에서 중요한 역할을 한다. 이 프로젝트에서는 getpid(), getuid() 등과 같은 시스템 호출을 시뮬레이션하여 프로세스나 사용자 정보를 얻고, 이를 테스트하거나 출력하는 작업을 수행한다.

```
int syscall_result = getpid();  // 현재 프로세스 ID 가져오기
kernel_printf("시스템 호출 SYS_getpid 반환값: %d\n", syscall_result);
```
 위의 코드는 getpid() 시스템 호출을 통해 현재 실행 중인 프로세스의 ID를 출력한다. 이를 통해 프로세스의 상태를 관리하거나 디버깅에 사용할 수 있다.
 
#### 5.6 프로세스 및 멀티프로세스 처리
`kernel_engine.c`는 프로세스 관리에 중점을 둔 파일로, 단일 프로세스 및 다중 프로세스 생성 기능을 제공한다. 이는 병렬 작업을 수행하거나, 독립적인 프로세스에서 작업을 분리하여 처리할 때 유용하다.


----------

## 6. 개인 프로젝트 스마트포인터 C구현
##### 스마트 포인터 활용 관계 분석
 `스마트 포인터(SmartPtr)`는 클라이언트 관리와 동시성 제어를 위해 사용된다. 특히, 동적 메모리 할당 및 해제를 관리하고, 참조 카운트와 뮤텍스를 이용하여 안전한 메모리 관리와 스레드 동기화를 보장한다.
 `client_infos[csock] = create_smart_ptr(client_info);` 구문을 포함한 전체 구조를 상세히 설명하도록 한다.
 
 
#### 6.1. 스마트 포인터 구조와 관리 방식

스마트 포인터는 SmartPtr 구조체로 정의되며, 다음과 같은 요소로 구성된다.

    void *ptr: 스마트 포인터가 가리키는 실제 데이터 (여기서는 클라이언트 정보)
    int *ref_count: 참조 카운트. 해당 포인터가 참조되는 횟수를 관리하며, 이 값이 0이 되면 메모리를 해제합니다.
    pthread_mutex_t *mutex: 동기화 메커니즘을 제공하는 뮤텍스. 여러 스레드에서 안전하게 참조 카운트를 수정하거나 메모리를 해제할 수 있도록 보장합니다.

```
typedef struct SmartPtr {
    void *ptr;
    int *ref_count;
    pthread_mutex_t *mutex;
} SmartPtr;
```

#### 6.1 스마트 포인터 생성 함수: create_smart_ptr()

create_smart_ptr() 함수는 새로운 스마트 포인터를 생성하고 초기화한다. 중요한 단계는 참조 카운트를 1로 초기화하고, 메모리 동기화를 위해 뮤텍스를 할당 및 초기화할 수 있다.

```
SmartPtr create_smart_ptr(void *ptr) {
    SmartPtr sp;
    sp.ptr = ptr;
    sp.ref_count = (int *)malloc(sizeof(int));  // 참조 카운트 메모리 할당
    *(sp.ref_count) = 1;  // 참조 카운트 초기값 설정
    sp.mutex = (pthread_mutex_t *)malloc(sizeof(pthread_mutex_t));  // 뮤텍스 메모리 할당 및 초기화
    pthread_mutex_init(sp.mutex, NULL);
    return sp;
}
```

#### 가변인자 적용 예
`create_smart_ptr` 함수는 메모리 할당 및 참조 카운트 초기화를 수행한다.
이 함수는 다양한 데이터 타입을 초기화할 수 있도록 설계하였다.
`가변 인자(va_list)`를 사용하여, 함수 호출 시 전달된 데이터 타입에 맞춰 스마트 포인터 내부의 값을 설정할 수 있다. (int 타입 또는 문자열 처리)
- 뮤텍스 초기화: `pthread_mutex_init` 함수로 뮤텍스를 초기화하여 다중 스레드 환경에서 안전하게 사용할 수 있다.

```
SmartPtr create_smart_ptr(size_t size, ...) {
    SmartPtr sp;
    sp.ptr = malloc(size);
    sp.ref_count = (int *)malloc(sizeof(int));
    *(sp.ref_count) = 1;
    sp.mutex = (pthread_mutex_t *)malloc(sizeof(pthread_mutex_t));
    pthread_mutex_init(sp.mutex, NULL);

    // 가변 인자를 통해 초기화
    va_list args;
    va_start(args, size);

    if (size == sizeof(int)) {
        int value = va_arg(args, int);
        *(int *)sp.ptr = value;
    } else if (size == sizeof(char) * MAX_STRING_SIZE) {
        const char *str = va_arg(args, const char *);
        strncpy((char *)sp.ptr, str, MAX_STRING_SIZE);
    }

    va_end(args);
    return sp;
}
```

#### 6.2 스마트 포인터 참조 증가 함수: retain()

`retain()` 함수는 스마트 포인터의 참조 카운트를 1 증가시킨다. 이때 뮤텍스를 사용하여 스레드 안전하게 참조 카운트를 조작할 수 있다.

```
void retain(SmartPtr *sp) {
    pthread_mutex_lock(sp->mutex);  // 뮤텍스 잠금
    (*(sp->ref_count))++;  // 참조 카운트 증가
    pthread_mutex_unlock(sp->mutex);  // 뮤텍스 해제
}
```

#### 6.3 스마트 포인터 참조 해제 함수: release()

`release()` 함수는 참조 카운트를 감소시키고, 참조 카운트가 0이 되면 메모리를 해제한다. 이 과정에서도 뮤텍스를 사용하여 스레드 간 경쟁 조건을 방지하고, 안전하게 메모리를 해제할 수 있다.

```
void release(SmartPtr *sp) {
    int should_free = 0;
    pthread_mutex_lock(sp->mutex);  // 뮤텍스 잠금
    (*(sp->ref_count))--;  // 참조 카운트 감소
    if (*(sp->ref_count) == 0) {  // 참조 카운트가 0이면 메모리 해제 플래그 설정
        should_free = 1;
    }
    pthread_mutex_unlock(sp->mutex);  // 뮤텍스 해제

    if (should_free) {  // 참조 카운트가 0인 경우, 메모리 해제
        free(sp->ptr);
        free(sp->ref_count);
        pthread_mutex_destroy(sp->mutex);
        free(sp->mutex);
    }
}
```

#### 6.4 내가 만든 `스마트포인터`와 일반 포인터와의 차이
>`SmartPtr`을 사용하는 것과 일반 포인터를 사용하는 것 사이에는 몇 가지 중요한 차이점이 존재한다. 
이 차이점들은 주로 메모리 관리, 동시성 문제 해결, 그리고 코드의 안전성 및 유지보수성 측면에서 나타난다.

**1. 메모리 관리**
- 기본 포인터: 일반 포인터를 사용할 때는 메모리 할당(malloc)과 해제(free)를 직접 처리해야 합니다. 이로 인해 메모리 누수(memory leak)나 중복 해제(double free)와 같은 문제가 발생할 가능성이 큽니다. 메모리 해제를 놓치면 사용하지 않는 메모리가 계속 남아있어 프로그램이 점점 더 많은 메모리를 소비하게 된다.

- SmartPtr: SmartPtr은 참조 카운트(ref_count)를 이용해 메모리를 관리한다. 여러 스레드가 동일한 자원을 참조할 때, 참조 카운트를 증가시키고, 더 이상 참조되지 않으면 자동으로 메모리를 해제한다. 사용자는 메모리 해제 타이밍을 신경 쓸 필요 없이 자동으로 처리된다.

**2. 참조 카운트**
  -기본 포인터: 일반 포인터는 메모리를 공유하는 여러 포인터들 사이에서 그 메모리가 몇 개의 포인터에 의해 참조되고 있는지 추적하지 않는다. 여러 곳에서 동일한 메모리를 가리키는 포인터가 있을 때, 어떤 포인터가 free를 호출해야 하는지 명확하지 않으면 문제가 발생할 수 있다.
  - SmartPtr: SmartPtr은 참조 카운트를 이용해 몇 개의 포인터가 해당 메모리를 참조하고 있는지를 추적한다. 참조 카운트가 0이 되면 메모리를 자동으로 해제한다. 이를 통해 여러 곳에서 메모리를 참조할 때도 안전하게 사용할 수 있다.

**3. 동시성 안전성 (스레드 안전성)**
- 기본 포인터: 일반 포인터를 멀티스레드 환경에서 사용할 경우 동기화 없이 메모리를 읽거나 쓸 수 있다. 이것은 여러 스레드에서 동시에 같은 메모리를 액세스할 때 경쟁 상태(race condition)를 일으킬 수 있으며, 이로 인해 예기치 않은 동작이나 충돌이 발생할 수 있다.
- SmartPtr: SmartPtr은 뮤텍스(pthread_mutex_t)를 사용하여 멀티스레드 환경에서 참조 카운트를 변경하는 작업이 동기화된다. 여러 스레드가 동시에 참조 카운트를 수정하더라도 안전하게 메모리를 관리할 수 있다.

**4. 안전성 및 예외 처리**
- 기본 포인터: 일반 포인터는 예외가 발생하거나 함수가 비정상적으로 종료되면, 메모리 해제가 누락될 수 있다. 이로 인해 메모리 누수가 발생할 수 있으며, 오류를 처리하는 과정에서 추가적인 복잡성이 필요하다.
- SmartPtr: SmartPtr은 메모리 해제를 참조 카운트가 0이 되는 시점에서 자동으로 처리하므로, 예외가 발생하더라도 메모리를 안전하게 해제할 수 있다. 참조 카운트 기반 메모리 관리는 메모리 누수나 잘못된 메모리 접근으로부터 보호해준다.

**5. 코드 유지보수성**
- 기본 포인터: 기본 포인터를 사용할 경우, 메모리를 언제 할당하고 언제 해제해야 하는지 사용자가 명확하게 관리해야 하므로 코드가 복잡해지고, 유지보수가 어려워질 수 있다.
- SmartPtr: SmartPtr은 자동 메모리 관리를 제공하여 코드의 복잡성을 줄여줍니다. 메모리 해제를 자동으로 처리하기 때문에, 메모리 관련 문제를 추적하거나 디버깅할 일이 줄어들어 코드의 가독성과 유지보수성이 향상될 수 있다.

**정리**
- 메모리 관리: 기본 포인터는 사용자가 메모리 해제를 직접 관리해야 하고, SmartPtr은 참조 카운트를 사용해 자동으로 관리한다.
- 동시성 문제: 기본 포인터는 멀티스레드 환경에서 안전하지 않지만, SmartPtr은 뮤텍스를 사용해 스레드 안전성을 제공한다.
- 안전성: SmartPtr은 참조 카운트로 메모리 해제를 자동으로 처리해 메모리 누수와 예외 안전성을 보장한다.
- 유지보수성: SmartPtr은 자동화된 메모리 관리로 코드 복잡성을 줄이고 유지보수를 쉽게 만들 수 있다.


**결론적으로, 기본 포인터는 직접적인 메모리 관리와 동시성 문제가 있을 수 있는 반면, SmartPtr은 이러한 문제를 해결해주는 고급 메모리 관리 도구로, 특히 멀티스레드 환경에서 더 안전하고 효율적으로 사용할 수 있다.
**

----------

### 스마트 포인터 활용 사례: 클라이언트 정보 관리

 `client_infos[csock] = create_smart_ptr(client_info);` 코드가 클라이언트 정보를 어떻게 관리하는지 살펴보도록 한다. 이 코드는 클라이언트별로 스마트 포인터를 생성하고, 클라이언트의 데이터를 안전하게 관리할 수 있도록 할 수 있다.

#### 6.4 클라이언트 정보 구조체: ClientInfo
ClientInfo 구조체는 각각의 클라이언트에 대한 정보를 담고 있다. 이 구조체는 스마트 포인터로 관리되어 참조 카운트를 통해 메모리를 자동으로 관리한다.

```
typedef struct {
    int client_fd;  // 클라이언트 소켓 파일 디스크립터
    int client_id;  // 클라이언트 ID
    pthread_mutex_t *client_mutex;  // 클라이언트별 뮤텍스
} ClientInfo;
```

#### 6.5 클라이언트 연결 시 스마트 포인터 생성

서버는 새 클라이언트가 연결될 때마다 `accept()`로 클라이언트 소켓을 받아들인 후, 클라이언트 정보를 동적으로 할당하고 스마트 포인터로 관리할 수 있다.

```
ClientInfo *client_info = (ClientInfo *)malloc(sizeof(ClientInfo));
client_info->client_fd = csock;
client_info->client_id = client_count++;
client_info->client_mutex = client_mutex;  // 클라이언트별 뮤텍스

client_infos[csock] = create_smart_ptr(client_info);  // 클라이언트 정보 스마트 포인터로 관리
```

 이 코드에서 create_smart_ptr(client_info)는 동적 할당된 클라이언트 정보를 스마트 포인터로 래핑하여 참조 카운트를 관리하게 만들 수 있다.
 이로 인해 메모리 해제를 자동화하고, 다중 스레드 환경에서 안전하게 접근할 수 있다.

#### 6.6 스마트 포인터 기반 동기화

클라이언트 연결 및 통신 중에는 각 클라이언트에 대해 뮤텍스를 활용한 동기화가 필요하다. `client_info->client_mutex`는 클라이언트별로 동기화를 보장하기 위해 사용된다.

```
pthread_mutex_lock(client_info->client_mutex);
printf("뮤텍스 %d 호출\n", client_info->client_id);
pthread_mutex_unlock(client_info->client_mutex);
```

이 코드는 클라이언트의 뮤텍스를 잠그고, 해당 클라이언트에 대해 안전하게 작업을 수행한 후, 뮤텍스를 해제한다. 
이를 통해 데이터 경합을 방지하고, 동시성 문제를 해결할 수 있었다.

#### 6.7 클라이언트 종료 시 스마트 포인터 해제

클라이언트가 연결을 종료하면, 해당 클라이언트 정보를 참조 카운트에 따라 메모리에서 해제한다. 이는 스마트 포인터의 `release()` 함수에서 처리할 수 있다.

```
release(&client_infos[csock]);  // 참조 카운트가 0이 되면 메모리 해제
```
`release()` 함수는 참조 카운트를 감소시키고, 카운트가 0이 되면 메모리를 안전하게 해제한다. 이 과정에서 뮤텍스를 사용하여 다른 스레드와의 경합을 방지할 수 있다.

#### 6.8 스마트 포인터와 클라이언트 관리의 장점

_스마트 포인터를 활용한 클라이언트 관리의 핵심적인 장점은 다음과 같았다._

1. 자동 메모리 관리: 스마트 포인터는 참조 카운트를 사용하여 메모리를 자동으로 해제합니다. 따라서 메모리 누수 없이 안전하게 클라이언트 정보를 관리할 수 있습니다.
2. 동시성 문제 해결: 뮤텍스를 사용하여 여러 스레드가 동시에 클라이언트 정보를 수정할 때 발생할 수 있는 데이터 경합 문제를 방지합니다. 이를 통해 데이터의 일관성을 유지할 수 있습니다.
3. 확장성: 스마트 포인터를 통해 클라이언트가 증가해도 각각의 클라이언트에 대해 독립적으로 메모리와 자원을 관리할 수 있어 확장성을 보장합니다.
4. 코드의 단순화: 스마트 포인터는 참조 카운트와 동적 메모리 관리를 자동화하여 메모리 관리 코드의 복잡성을 줄이고, 오류 가능성을 감소시킵니다.

#### 6.9 스마트 포인터 요약

> 스마트 포인터를 사용하여 클라이언트 정보를 안전하게 관리하는 것은 메모리 관리와 동시성 제어라는 두 가지 핵심 문제를 해결하는데에 도움이 된 것 같다. 특히 강사님께서 내주신 채팅 프로그램 숙제를 풀이할 때에 멀티스레드 환경에서 참조 카운트와 뮤텍스를 결합하여 데이터를 안전하게 관리하고, 자동화된 메모리 해제를 보장을 하면서 보다 편리하게 만들 수 있었다.

#### 개인 프로젝트 적용 결과는 성공적이었다. 다만, TCP나 UDP를 적용한 채팅 프로그램은 아직이다.
![](https://velog.velcdn.com/images/laura_vdea/post/f7848c1e-c593-46aa-a730-12c566a7c352/image.png)

**make 시작** and `td_kernel_engine.exec` **실행파일 도출**
```
azabell@azabell-Vivobook:~/Desktop/workspace_qt/QT_Kernel_OS/C_lib$ make
gcc -std=c11 -Wall -Wextra -Iinclude -Iinclude_printf -Wformat -Werror -Wno-unused-label -
..... 내용이 아주 많음
Creating universal static library kernel_lib.a
Creating td_kernel_lib.a static library
Building td_kernel_engine executable
gcc -std=c11 -Wall -Wextra -Iinclude -Iinclude_printf -Wformat -Werror -Wno-unused-label -Wno-unused-function -Wno-unused-variable -Wno-format-truncation -fcommon -o td_kernel_engine.exec src/td_kernel_engine.c src/kernel_asm.x86_64.o src/kernel_engine.x86_64.o src/kernel_lib.x86_64.o src/kernel_printf.x86_64.o kernel_printf.a td_kernel_lib.a
rm kernel_asm_1.arm64.a kernel_asm_6.x86_64.a kernel_asm_7.x86_64.o kernel_asm_2.x86_64.a kernel_asm_3.arm64.a kernel_asm_6.x86_64.o kernel_asm_4.x86_64.a kernel_asm_5.x86_64.o kernel_asm_1.arm64.o kernel_asm_7.x86_64.a kernel_asm_2.arm64.a kernel_asm_2.x86_64.o kernel_asm_3.x86_64.o kernel_asm_5.x86_64.a kernel_asm_3.x86_64.a kernel_asm_2.arm64.o kernel_asm_1.x86_64.o kernel_asm_7.arm64.o kernel_asm_6.arm64.o kernel_asm_7.arm64.a kernel_asm_5.arm64.o kernel_asm_4.x86_64.o kernel_asm_6.arm64.a kernel_asm_1.x86_64.a kernel_asm_4.arm64.a kernel_asm_4.arm64.o kernel_asm_3.arm64.o kernel_asm_5.arm64.a
```

`td_kernel_engine.exec` **시작**
```
azabell@azabell-Vivobook:~/Desktop/workspace_qt/QT_Kernel_OS/C_lib$ ./td_kernel_engine.exec 
커널 메인 함수 시작
커널 환경 설정 중
IPC를 위한 소켓 페어 생성 중
소켓 페어 생성됨: FD1=3, FD2=4
자식 프로세스 생성 중
부모 프로세스 시작됨
부모 프로세스가 자식과 통신 중
자식 프로세스 시작됨
자식 프로세스 작업 수행 중
부모 프로세스에서 받은 메시지: 부모 프로세스에서 보냄!
자식 프로세스가 테스트를 시작합니다...
파일 열기 성공: non_existent_file.txt
파일 내용: Hello Kernel OS!
파일 내용: Hello Kernel OS!
파일 내용: Hello Kernel OS!
파일 내용: Hello Kernel OS!
파일 내용: Hello Kernel OS!
파일 내용: HELLO VEDA ACADEMY!!자식 프로세스에서 특정 작업 실행 중
Thread 1: 시작
Thread 1: 종료
자식 프로세스 작업 완료
자식 프로세스 종료
파일 열기 성공: non_existent_file.txt
파일 내용: Hello Kernel OS!
파일 내용: Hello Kernel OS!
파일 내용: Hello Kernel OS!
파일 내용: Hello Kernel OS!
파일 내용: Hello Kernel OS!
파일 내용: HELLO VEDA ACADEMY!!부모 프로세스에서 멀티스레딩 테스트 수행 중
Thread 1: 시작
Thread 2: 시작
Thread 3: 시작
Thread 1: 종료
Thread 2: 종료
Thread 3: 종료
부모 프로세스에서 스마트 포인터 연산 테스트 중
Smart pointer retained (ref_count: 2)
Smart pointer released (ref_count: 1)
Reference count is not 0, not freeing memory
Mutex destroyed
Smart pointer released (ref_count: 0)
Reference count is 0, freeing memory...
Memory must be freed
부모 프로세스에서 세마포어 동기화 테스트 중
세마포어 대기
세마포어 획득
세마포어 대기
세마포어 해제
세마포어 획득
세마포어 해제
부모 프로세스에서 뮤텍스 동기화 테스트 중
뮤텍스 대기
뮤텍스 획득
뮤텍스 대기
뮤텍스 해제
뮤텍스 해제
뮤텍스 획득
뮤텍스 해제
뮤텍스 해제
부모 프로세스에서 고급 오류 처리 테스트 중
ERROR [?UNKNOWN? Success] 널 포인터 감지, 복구 시도 중

ERROR [ENOENT No such file or directory] 유효하지 않은 파일 열기 실패, 계속 진행

부모 프로세스에서 프로세스 관리 시뮬레이션 중
작업자 프로세스 시작됨
Process 18137: 시작
Process 18137: 종료
작업자 프로세스 종료
부모 프로세스에서 메모리 관리 시뮬레이션 중
메모리 할당됨, 주소: 0x7439ede0
메모리 해제됨
부모 프로세스에서 장치 I/O 시뮬레이션 중
장치 I/O 완료
부모 프로세스에서 파일 시스템 작업 시뮬레이션 중
테스트 파일 생성됨: /tmp/kernel_test_file
테스트 파일에서 읽음: 커널 테스트 데이터
테스트 파일 삭제됨: /tmp/kernel_test_file
부모 프로세스에서 네트워크 작업 시뮬레이션 중
네트워크 작업 시뮬레이션 완료
부모 프로세스에서 시스템 호출 시뮬레이션 중
시스템 호출 SYS_getpid 반환값: 18110
시스템 호출 SYS_getuid 반환값: 1000
부모 프로세스에서 하드웨어 인터럽트 시뮬레이션 중
하드웨어 인터럽트 시뮬레이션 완료
부모 프로세스에서 커널 패닉 시뮬레이션 중
커널 패닉 시뮬레이션 회피됨
부모 프로세스에서 최종 정리 작업 수행 중
최종 정리 작업 완료
커널 메인 함수 종료
```
--------------------

## 7. BSP 진도가 모두 끝난 뒤에 느낀 점

이번 네트워크 프로그래밍 학습을 통해, epoll과 select를 사용한 비동기 통신의 개념과 실제 구현 방법을 익혔다. 서버는 epoll을 통해 여러 클라이언트를 효율적으로 관리하고, 클라이언트는 `select`로 사용자 입력과 서버 메시지를 처리하는게 좋았다. 

 또한, 내가 직접 만든 `스마트 포인터`를 사용해 자원 관리를 자동화하여 메모리 누수를 방지할 수 있었다.(약간 조크식 말 장난으로 `스마트 포인터`다)

 이번 주 학습에서 배운 내용을 바탕으로, 앞으로의 학습과 프로젝트에서 목표를 구체적으로 세우고 실천하고자 한다. 특히 리눅스 BSP 팀에 들어가고자 하는 목표에 맞추어, 리눅스 커널과 시스템 프로그래밍에 대한 깊이 있는 학습을 계속 이어 나가겠다.

**리눅스 커널 프로그래밍 심화 학습**
> 다음 주에는 리눅스 커널 모듈 작성을 중점적으로 학습할 계획이다. 특히, 커널 모듈이 어떻게 하드웨어 리소스를 제어하고, 프로세스와 시스템 자원을 관리하는지에 대해 실습해볼 예정이다.
또한, 커널 레벨에서의 프로세스 간 통신(IPC) 방법을 심도 있게 다뤄보고, 동시성 문제와 자원 관리에 대한 문제 해결 능력을 키우겠다.

**프로세스 간 통신(IPC) 심화**
> 이번 주에 배운 멀티스레드와 동기화 개념을 더 깊이 이해하고, 프로세스 간의 데이터 통신에 대해 학습할 계획이다. 파이프(pipe), 소켓(socket), 그리고 공유 메모리와 같은 IPC 기술을 배워, 프로세스 간 효율적인 통신 방법을 이해하고 이를 구현해보는 것을 목표로 하고 있다.
        특히, 동시성 제어와 데이터 무결성을 보장하는 방법에 대해 실습을 통해 숙달하겠다.

**리눅스 파일 시스템 조작 및 확장성 학습**
> 파일 디스크립터와 inode와 같은 리눅스 파일 시스템 구조에 대해 더 깊이 있는 학습을 진행할 예정이다. 특히, 파일 디스크립터와 파일 잠금 등을 효율적으로 관리하는 방법에 대해 실습을 통해 익히고자 한다.
파일 메타데이터와 파일 접근 권한 관리를 통해 리눅스에서의 보안과 자원 관리를 어떻게 설계할지 탐구해보고, 실무에서 요구되는 리눅스 시스템 프로그래밍 실력을 쌓겠다.

**QT_Kernel_OS 프로젝트 연계**
> 현재 진행 중인 QT_Kernel_OS 프로젝트에서 배운 내용을 접목시켜, 멀티스레드 환경에서의 동시성 문제 해결과 프로세스 관리를 더욱 효율적으로 설계하겠다.
스마트 포인터를 활용한 자원 관리와 커널 엔진의 프로세스 제어 기술을 프로젝트에 통합하여 안정성과 확장성을 보장할 수 있는 시스템을 구축할 계획이다.

**🌱 앞으로의 학습 방향**

이번 주 학습을 통해 리눅스 시스템 프로그래밍의 기초에서부터 멀티스레드 동기화, 프로세스 관리, 그리고 스마트 포인터 활용까지 심도 있게 학습할 수 있었다. 앞으로는 더 나아가 리눅스 커널과 저수준 프로그래밍 기술을 익히고, 실무에서 활용할 수 있는 시스템 프로그래밍 실력을 갖추려고 한다.

 나는 신입을 정말 잘 안받는 리눅스 BSP 팀에 들어가고자 하는 목표를 달성하기 위해, 지금까지 배운 내용에 더해 리눅스 커널 모듈과 프로세스 간 통신에 대해 더 깊이 있게 학습하고 실습할 것이다.

 이번 주 학습을 통해 얻은 가장 큰 교훈은, 시스템 프로그래밍에서 중요한 것은 안정성과 확장성을 동시에 고려한 설계라는 점이다. 멀티스레드와 멀티프로세스 환경에서 동시성 문제를 해결하고, 자원을 효율적으로 관리하는 것이 얼마나 중요한지 다시 한 번 깨달았다.

------------------------

## 8. 마무리
### ✌️ 마무리 정리
**😄 Liked**
이번 주 한화비전 VEDA 아카데미에서 리눅스 환경의 네트워크 프로그래밍을 학습하며 특히 좋았던 점은 epoll과 `select`를 사용해 다중 클라이언트와 서버 간 비동기 통신을 구현한 것입니다. 네트워크 프로그램을 직접 코딩하면서 비동기 통신의 원리를 이해하고 실무적인 개념을 깊이 있게 학습할 수 있었습니다. 
 특히 스마트 포인터를 직접 구현해보며 뮤텍스와 세마포어를 잘 사용한다면 메모리 관리와 자원 해제를 체계적으로 처리할 수 있다는 점이 인상 깊었습니다.🙏

**😄 Learned**
이번 주 학습에서 주목할 만한 점은 네트워크 프로그래밍과 동기화 기법에 대한 이해가 크게 향상된 것입니다🙏

1. 네트워크 프로그래밍: epoll과 select를 사용해 다중 클라이언트와 서버 간 비동기 통신을 처리하는 방법을 익혔습니다. 또한, pthread를 통해 클라이언트 간의 동기화를 처리하고, 스마트 포인터로 자원 관리까지 구현해 보았습니다.
2. 스마트 포인터 구현: 스마트 포인터를 C로 직접 구현하며 참조 카운트를 관리하는 방식을 배웠습니다. 메모리 관리와 동시성 제어를 동시에 해결할 수 있었습니다.
3. 리눅스 프로세스 관리: 리눅스에서 프로세스 생성, 통신, 종료와 같은 관리 작업을 학습했고, 파일 시스템 조작을 통해 파일 디스크립터와 파일 접근 권한 관리 방법을 익혔습니다.

**😄 Lacked**
 이번 주 학습에서 아쉬웠던 점은 파일 시스템 구조와 inode에 대한 깊이 있는 내용을 충분히 다루지 못했다는 것입니다. 특히, 파일 메타데이터 관리와 시스템 자원에 대한 심화 학습이 부족했습니다. 프로세스 간 통신에서 다룬 기본 개념 외에도 복잡한 시나리오에서 동시성 문제와 파일 시스템의 병렬 접근에 대한 실습이 더 필요할 것 같습니다.🙏
 
**😄 Longed for**
 다음 주에는 이번 학습 내용을 확장해 리눅스 커널 모듈 작성과 저수준 프로그래밍을 중점적으로 다룰 계획입니다. 특히, **프로세스 간 통신(IPC)**과 동시성 제어 기술을 심화 학습해 리눅스 시스템 프로그래밍의 핵심 개념을 더 깊이 있게 이해할 것입니다.🙏
 

### ✌️ 회고 및 다짐
**- 이번 주까지 어떤 것을 배웠나요?**
 이번 주에는 한화비전 VEDA 아카데미의 커리큘럼을 통해 네트워크 프로그래밍과 동기화에 중점을 두었습니다. 클라이언트-서버 통신에서 epoll과 select를 이용한 비동기 통신을 학습하였고, C언어로 스마트 포인터를 직접 구현하여 자원 관리 및 동시성 제어의 중요성을 체감했습니다. 프로세스 생성과 자원 해제를 통한 시스템 프로그래밍에 대한 개념적 토대도 쌓을 수 있었습니다.🙏

**- 이번 주에 겪은 시행착오나 어려웠던 점은 무엇인가요?**
 스레드 기반의 프로그램을 작성하는 과정에서 동시성 문제와 뮤텍스 잠금과 관련한 예상치 못한 동작을 해결하는 것이 어려웠습니다. 시그널 처리 부분에서도 시그널 핸들러 설정에서 혼란을 겪었으며, 디버깅 과정이 상당한 시간을 필요로 했습니다. 특히 파일 시스템의 동시 접근에 대한 부분은 심도 있게 다루지 못해 아쉬웠습니다.🙏

**- 앞으로 적용해야겠다고 느낀 점이 있다면 무엇인가요?**
 앞으로는 리눅스에서 발생할 수 있는 동시성 문제를 해결하기 위한 실습을 더 많이 해야겠다고 느꼈습니다. 특히 프로세스 간 통신에서 발생할 수 있는 자원 경합 문제를 처리하는 방법과 파일 시스템의 동기화를 더 깊이 있게 학습할 필요가 있습니다. 이를 위해 더 많은 실습을 통해 문제 해결 능력을 키우고자 합니다.🙏
 
**- 다음 학습을 위한 다짐이나 목표를 공유해주세요.**
 다음 주에는 리눅스 커널 프로그래밍과 IPC 심화 학습에 중점을 두겠습니다. 특히 QT_Kernel_OS 프로젝트에 배운 내용을 적용해 프로세스 관리와 동시성 제어의 효율성을 높이는 방법을 탐구할 것입니다. 꾸준한 학습을 통해 리눅스 BSP 팀에 합류하기 위한 준비를 차근차근 해나가고, 시스템 프로그래밍 실력을 다지도록 하겠습니다.
 다만, 걱정되는 점은 `리눅스 커널 개발 BSP`팀은 한화에서 `to`가 없다는 점인데, 그래도 최대한 노력을 다해보려고 합니다.🙏
 
 
---------------------

VEDA 바로가기 : `www.vedacademy.co.kr`
VEDA(한화비전 아카데미) 영상으로 확인하기 : `https://url.kr/zy9afd`
본 후기는 VEDA(한화비전 아카데미) 첫번째 학습 기록으로 작성되었습니다.
교육기간 : `2024.07.15~2024.12.16`
